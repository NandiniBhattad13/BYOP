{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOuLiLn7YSdnfb7TutJeKqh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NandiniBhattad13/BYOP/blob/main/BYOPgradio.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio\n",
        "!pip install datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JidW_rR2-RHZ",
        "outputId": "7a89f1ad-cf00-49ef-fccc-17fc8a63d209"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gradio in /usr/local/lib/python3.10/dist-packages (4.14.0)\n",
            "Requirement already satisfied: aiofiles<24.0,>=22.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (23.2.1)\n",
            "Requirement already satisfied: altair<6.0,>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.2.2)\n",
            "Requirement already satisfied: fastapi in /usr/local/lib/python3.10/dist-packages (from gradio) (0.109.0)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.10/dist-packages (from gradio) (0.3.1)\n",
            "Requirement already satisfied: gradio-client==0.8.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.8.0)\n",
            "Requirement already satisfied: httpx in /usr/local/lib/python3.10/dist-packages (from gradio) (0.26.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.20.2)\n",
            "Requirement already satisfied: importlib-resources<7.0,>=1.3 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.1.1)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.1.2)\n",
            "Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.1.3)\n",
            "Requirement already satisfied: matplotlib~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\n",
            "Requirement already satisfied: numpy~=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.23.5)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.9.10)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from gradio) (23.2)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.5.3)\n",
            "Requirement already satisfied: pillow<11.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (9.4.0)\n",
            "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.5.3)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.10/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart in /usr/local/lib/python3.10/dist-packages (from gradio) (0.0.6)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.0.1)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: tomlkit==0.12.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.12.0)\n",
            "Requirement already satisfied: typer[all]<1.0,>=0.9 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.9.0)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.9.0)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.25.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from gradio-client==0.8.0->gradio) (2023.6.0)\n",
            "Requirement already satisfied: websockets<12.0,>=10.0 in /usr/local/lib/python3.10/dist-packages (from gradio-client==0.8.0->gradio) (11.0.3)\n",
            "Requirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio) (0.4)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio) (4.19.2)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio) (0.12.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio) (3.13.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio) (4.66.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (4.47.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2023.3.post1)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.14.6 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (2.14.6)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer[all]<1.0,>=0.9->gradio) (8.1.7)\n",
            "Requirement already satisfied: colorama<0.5.0,>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from typer[all]<1.0,>=0.9->gradio) (0.4.6)\n",
            "Requirement already satisfied: shellingham<2.0.0,>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer[all]<1.0,>=0.9->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich<14.0.0,>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer[all]<1.0,>=0.9->gradio) (13.7.0)\n",
            "Requirement already satisfied: h11>=0.8 in /usr/local/lib/python3.10/dist-packages (from uvicorn>=0.14.0->gradio) (0.14.0)\n",
            "Requirement already satisfied: starlette<0.36.0,>=0.35.0 in /usr/local/lib/python3.10/dist-packages (from fastapi->gradio) (0.35.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx->gradio) (3.7.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx->gradio) (2023.11.17)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx->gradio) (1.0.2)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.10/dist-packages (from httpx->gradio) (3.6)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx->gradio) (1.3.0)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (23.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (0.32.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio) (0.16.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib~=3.0->gradio) (1.16.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<14.0.0,>=10.11.0->typer[all]<1.0,>=0.9->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich<14.0.0,>=10.11.0->typer[all]<1.0,>=0.9->gradio) (2.16.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx->gradio) (1.2.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.19.3->gradio) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.19.3->gradio) (2.0.7)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich<14.0.0,>=10.11.0->typer[all]<1.0,>=0.9->gradio) (0.1.2)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.16.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.13.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.23.5)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (10.0.1)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: fsspec[http]<=2023.10.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.4 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.20.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.4->datasets) (4.9.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2023.11.17)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.3.post1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "R_RI1D9a-J-0"
      },
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "import re\n",
        "\n",
        "from transformers import pipeline"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import datasets\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import tensorflow as tf\n",
        "from tqdm import tqdm\n",
        "from transformers import pipeline\n",
        "from transformers import create_optimizer\n",
        "from transformers import AutoTokenizer\n",
        "from transformers import BertTokenizer\n",
        "from transformers import BertModel\n",
        "from datasets import load_dataset\n",
        "from transformers import TFAutoModelForSequenceClassification\n",
        "from transformers.keras_callbacks import KerasMetricCallback\n",
        "from transformers import DataCollatorWithPadding"
      ],
      "metadata": {
        "id": "iyVma2NOAR91"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def sentiment(opn,text):\n",
        "\n",
        " if(opn == \"Emotion\"):\n",
        "    split_name = \"test\"\n",
        "    classifier = pipeline(task=\"text-classification\", model=\"SamLowe/roberta-base-go_emotions\", top_k=None)\n",
        "    model_output = classifier(text)\n",
        "    return model_output[0][0]['label']\n",
        "\n",
        " elif(opn == \"AI or not\"):\n",
        "   classifier = pipeline(task=\"text-classification\", model=\"Lau123/detect_ai_generated_text\", top_k=None)\n",
        "   model_output = classifier(text)\n",
        "   s = model_output[0][0]['score']\n",
        "   if(s>0.6):\n",
        "     ans=\"AI\"\n",
        "   else:\n",
        "     ans=\"HUMAN\"\n",
        "   return(ans)\n",
        "\n",
        " elif(opn == \"POS or NEG\"):\n",
        "\n",
        "    imdb = load_dataset(\"imdb\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
        "    def preprocess_function(examples):\n",
        "      return tokenizer(examples[\"text\"], truncation=True)\n",
        "    tokenized_imdb = imdb.map(preprocess_function, batched=True)\n",
        "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"tf\")\n",
        "    id2label = {0: \"NEGATIVE\", 1: \"POSITIVE\"}\n",
        "    label2id = {\"NEGATIVE\": 0, \"POSITIVE\": 1}\n",
        "    from transformers import create_optimizer\n",
        "    batch_size = 16\n",
        "    num_epochs = 5\n",
        "    batches_per_epoch = len(tokenized_imdb[\"train\"]) // batch_size\n",
        "    total_train_steps = int(batches_per_epoch * num_epochs)\n",
        "    optimizer, schedule = create_optimizer(init_lr=2e-5, num_warmup_steps=0, num_train_steps=total_train_steps)\n",
        "    model = TFAutoModelForSequenceClassification.from_pretrained(\n",
        "    \"distilbert-base-uncased\", num_labels=2, id2label=id2label, label2id=label2id)\n",
        "    tf_train_set = model.prepare_tf_dataset(\n",
        "    tokenized_imdb[\"train\"],\n",
        "    shuffle=True,\n",
        "    batch_size=16,\n",
        "    collate_fn=data_collator,)\n",
        "\n",
        "    tf_validation_set = model.prepare_tf_dataset(\n",
        "    tokenized_imdb[\"test\"],\n",
        "    shuffle=False,\n",
        "    batch_size=16,\n",
        "    collate_fn=data_collator,)\n",
        "\n",
        "    model.compile(optimizer=optimizer)\n",
        "    model.save_pretrained(\"saved_model\")\n",
        "    tokenizer.save_pretrained(\"saved_model\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"saved_model\")\n",
        "    inputs = tokenizer(text, return_tensors=\"tf\")\n",
        "    model = TFAutoModelForSequenceClassification.from_pretrained(\"saved_model\")\n",
        "    logits = model(**inputs).logits\n",
        "    predicted_class_id = int(tf.math.argmax(logits, axis=-1)[0])\n",
        "    predicted_class_id\n",
        "    return(id2label[predicted_class_id])\n",
        "\n",
        " else:\n",
        "\n",
        "   source_url = \"/content/bbc-text.csv\"\n",
        "   df = pd.read_csv(source_url)\n",
        "   df.groupby([\"category\"]).size()\n",
        "   tokenizer = BertTokenizer.from_pretrained(\"bert-base-cased\")\n",
        "   tokenizer= BertTokenizer.from_pretrained(\"bert-base-cased\")\n",
        "   labels = {\n",
        "      'business':0,\n",
        "      'entertainment':1,\n",
        "      'sport':2,\n",
        "      'tech':3,\n",
        "      'politics':4\n",
        "     }\n",
        "   class Dataset(torch.utils.data.Dataset):\n",
        "      def __init__(self,df):\n",
        "          #extract our labels from the df\n",
        "          self.labels = [labels[label] for label in df[\"category\"]]\n",
        "          #tokenize our texts to the format that BERT expects to get as input\n",
        "          self.texts = [tokenizer(text, padding='max_length', max_length=512, truncation=True,return_tensors=\"pt\") for text in df[\"text\"]]\n",
        "      def classes(self):\n",
        "          return self.labels\n",
        "\n",
        "      def __len__(self):\n",
        "          return len(self.labels)\n",
        "\n",
        "      #fetch a batch of labels\n",
        "      def get_batch_labels(self,indx):\n",
        "          return np.array(self.labels[indx])\n",
        "      # fetch a batch of texts\n",
        "      def get_batch_texts(self,indx):\n",
        "          return self.texts[indx]\n",
        "\n",
        "    #get an item with the texts and the label\n",
        "      def __getitem__(self,indx):\n",
        "        batch_texts = self.get_batch_texts(indx)\n",
        "        batch_y = self.get_batch_labels(indx)\n",
        "        return batch_texts, batch_y\n",
        "\n",
        "   df_train, df_valid,df_test = np.split(df.sample(frac=1,random_state=42),[int(.8*len(df)), int(.9*len(df))])\n",
        "\n",
        "   class BertClassifier(torch.nn.Module):\n",
        "    def __init__(self,dropout=0.5):\n",
        "        super(BertClassifier,self).__init__()\n",
        "\n",
        "        self.bert=BertModel.from_pretrained(\"bert-base-cased\")\n",
        "        self.dropout = torch.nn.Dropout(dropout)\n",
        "        # bert output a vector of size 768\n",
        "        self.lin = torch.nn.Linear(768,5)\n",
        "        self.relu = torch.nn.ReLU()\n",
        "    def forward(self,input_id,mask):\n",
        "        # as output, the bert model give us first the embedding vector of all the tokens of the sequence\n",
        "        # second we get the embedding vector of the CLS token.\n",
        "        # fot a classification task it's enough to use this embedding for our classifier\n",
        "        _,pooled_output = self.bert(input_ids= input_id,attention_mask = mask,return_dict = False)\n",
        "        dropout_output = self.dropout(pooled_output)\n",
        "        linear_output  = self.lin(dropout_output)\n",
        "        final_layer = self.relu(linear_output)\n",
        "        return final_layer\n",
        "\n",
        "   def train(model, train_data, val_data, learning_rate, epochs=5):\n",
        "    #creating a custom Dataset objects using the training and validation data\n",
        "    train, val = Dataset(train_data), Dataset(val_data)\n",
        "    #creating dataloaders\n",
        "    train_dataloader = torch.utils.data.DataLoader(train, batch_size=2, shuffle=True)\n",
        "    val_dataloader = torch.utils.data.DataLoader(val, batch_size=2)\n",
        "\n",
        "    use_cuda = torch.cuda.is_available()\n",
        "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr= learning_rate)\n",
        "\n",
        "    if use_cuda:\n",
        "            model = model.cuda()\n",
        "            criterion = criterion.cuda()\n",
        "\n",
        "    for epoch_num in range(epochs):\n",
        "\n",
        "            total_acc_train = 0\n",
        "            total_loss_train = 0\n",
        "\n",
        "            for train_input, train_label in tqdm(train_dataloader):\n",
        "                train_label = train_label.to(device)\n",
        "                mask = train_input['attention_mask'].to(device)\n",
        "                input_id = train_input['input_ids'].squeeze(1).to(device)\n",
        "                output = model(input_id, mask)\n",
        "                batch_loss = criterion(output, train_label)\n",
        "                total_loss_train += batch_loss.item()\n",
        "\n",
        "                acc = (output.argmax(dim=1) == train_label).sum().item()\n",
        "                total_acc_train += acc\n",
        "                model.zero_grad()\n",
        "                batch_loss.backward()\n",
        "                optimizer.step()\n",
        "            total_loss_val = 0\n",
        "\n",
        "   EPOCHS = 1\n",
        "   model = BertClassifier()\n",
        "   learning_rate = 1e-6\n",
        "   train(model, df_train, df_valid, learning_rate, EPOCHS)\n",
        "   device= torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "   inverse_labels = {v:k for k,v in labels.items()}\n",
        "   def predict(device,model,sentence):\n",
        "    sentence = sentence\n",
        "    sentence_input = tokenizer(sentence, padding='max_length', max_length=512, truncation=True,return_tensors=\"pt\").to(device)\n",
        "    input_id = sentence_input[\"input_ids\"]\n",
        "    mask = sentence_input[\"attention_mask\"]\n",
        "    output = model(input_id,mask)\n",
        "    predicted_class_label = output.argmax(dim=1)\n",
        "    predicted_class = inverse_labels[predicted_class_label.item()]\n",
        "    print(predicted_class)\n",
        "   predict(device,model,text)"
      ],
      "metadata": {
        "id": "BbZsxKHm-MEp"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "demo = gr.Interface(\n",
        "    fn=sentiment,\n",
        "    inputs=[\n",
        "        gr.Dropdown([\"AI or not\", \"POS or NEG\", \"Emotion\", \"category of text\"], label=\"Sentiment Analysis\", info=\"What type of sentiment do you want to analyze?\"),\n",
        "        gr.Textbox(label=\"Enter Text for Sentiment Analysis\")\n",
        "    ],\n",
        "    outputs=\"text\"\n",
        ")"
      ],
      "metadata": {
        "id": "WNo3Yldl--Br"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "demo.launch()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 626
        },
        "id": "uewDzY1k-nOa",
        "outputId": "100029da-666d-4148-fd80-963fbb22059f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Running on public URL: https://73a696a953cb486eba.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://73a696a953cb486eba.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    }
  ]
}